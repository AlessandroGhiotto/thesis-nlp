{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen/Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.basename(os.getcwd()) == \"mycode\":\n",
    "    os.chdir(\"..\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "model_inputs:  dict_keys(['input_ids', 'attention_mask'])\n",
      "model_inputs.input_ids.shape:  torch.Size([1, 39])\n",
      "\n",
      "generated_ids[0][:5]:  tensor([39814,     0,   362,  3460,  4128], device='cuda:0') ...\n",
      "generated_ids[0].shape:  torch.Size([107])\n",
      "\n",
      "response:\n",
      "Sure! A large language model (LLM) is an artificial intelligence that can generate human-like text based on the input it receives. These models have been trained on vast amounts of data and are capable of understanding natural language, generating coherent responses, and even composing original content. LLMs are widely used in various fields such as chatbots, virtual assistants, language translation, sentiment analysis, and more. They allow for efficient communication between humans and machines, enabling interactions that would be difficult or impossible with traditional human-computer interfaces.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False, # the output is not tokenized, we have just text\n",
    "    add_generation_prompt=True, # generation promp = <|im_start|>assistant at the end od the prompt\n",
    ")\n",
    "print(text)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "print(\"model_inputs: \", model_inputs.keys())\n",
    "print(\"model_inputs.input_ids.shape: \", model_inputs.input_ids.shape)\n",
    "print()\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# now the generated_ids containts also the input prompt. We remove it\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(\"generated_ids[0][:5]: \", generated_ids[0][:5], \"...\")\n",
    "print(\"generated_ids[0].shape: \", generated_ids[0].shape)\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\nresponse:\\n\", response, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "### Usage Recommendations\n",
    "\n",
    "We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\n",
    "\n",
    "1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n",
    "2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n",
    "3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n",
    "4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n",
    "\n",
    "Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151646,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Give me a short introduction to large language model.<｜Assistant｜><think>\n",
      "\n",
      "model_inputs:  dict_keys(['input_ids', 'attention_mask'])\n",
      "model_inputs.input_ids.shape:  torch.Size([1, 16])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_ids[0][:5]:  tensor([71486,    11,   773,   358,  1184], device='cuda:0') ...\n",
      "generated_ids[0].shape:  torch.Size([512])\n",
      "\n",
      "response:\n",
      "Alright, so I need to write a short introduction to a large language model. Hmm, okay, where do I start? I know LLMs are these big machines that can understand and generate human language, but I'm not exactly sure about all the details. Let me think about what I know and what I might not know.\n",
      "\n",
      "First, I remember that LLMs are a type of AI, right? They're designed to handle complex tasks like text generation, translation, summarization, etc. But what exactly do they do? Do they understand natural language, or do they process it differently? I think they can parse and generate text on their own, but maybe they have some limitations. Maybe they can't understand emotions or context as humans do, or maybe they're not as flexible as other AI models.\n",
      "\n",
      "I also recall that there are different types of LLMs. There's the generative models, which are designed to generate text, and there are also language models, which are a specific category. I think language models are the most common type, focusing on understanding and generating text. They use statistical models to predict the next word in a sequence based on the context.\n",
      "\n",
      "Now, how do these models work? I think they take in a sequence of words or phrases and use some kind of algorithm to predict the next word. This might involve looking at the frequency of certain words, the context around them, and maybe even some form of neural networks to capture patterns in the data. But I'm not sure how much they rely on human-like creativity or understanding of context.\n",
      "\n",
      "Another thing I'm a bit fuzzy on is the difference between language models and other AI models. For example, neural networks are another type of AI, but they don't have a strong understanding of language structure. LLMs, on the other hand, seem to excel at generating coherent and meaningful text, even when they don't have explicit instructions. That's interesting.\n",
      "\n",
      "I also wonder about the applications of LLMs. They're used in a variety of fields, from writing research papers to creating content for social media. They can also be used in healthcare, art, and even in creative writing. But I'm not sure how versatile they are or how much they're limited by their design. Maybe they're good for certain tasks but not others?\n",
      "\n",
      "I should also consider the challenges associated with LLMs. One thing that comes to mind is that they might not always produce perfectly human-like text. Sometimes, they might make mistakes or not handle certain nuances well.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True, # generation promp = <think>\\n at the end od the prompt\n",
    ")\n",
    "print(text)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "print(\"model_inputs: \", model_inputs.keys())\n",
    "print(\"model_inputs.input_ids.shape: \", model_inputs.input_ids.shape)\n",
    "print()\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(\"generated_ids[0][:5]: \", generated_ids[0][:5], \"...\")\n",
    "print(\"generated_ids[0].shape: \", generated_ids[0].shape)\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\nresponse:\\n\", response, sep=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
