{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen/Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.basename(os.getcwd()) == \"mycode\":\n",
    "    os.chdir(\"..\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "model_inputs:  dict_keys(['input_ids', 'attention_mask'])\n",
      "model_inputs.input_ids.shape:  torch.Size([1, 39])\n",
      "\n",
      "generated_ids[0][:5]:  tensor([39814,     0,   362,  3460,  4128], device='cuda:0') ...\n",
      "generated_ids[0].shape:  torch.Size([100])\n",
      "\n",
      "response:\n",
      "Sure! A large language model is an artificial intelligence (AI) system that can produce human-like text based on input from natural language. These models use large amounts of data and advanced algorithms to generate coherent and contextually appropriate responses to user queries or prompts. Large language models have been used in various applications such as chatbots, virtual assistants, machine translation, and more. They are designed to be more efficient than traditional AI systems due to their ability to process vast amounts of information quickly and accurately.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False, # the output is not tokenized, we have just text\n",
    "    add_generation_prompt=True, # generation promp = <|im_start|>assistant at the end od the prompt\n",
    ")\n",
    "print(text)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "print(\"model_inputs: \", model_inputs.keys())\n",
    "print(\"model_inputs.input_ids.shape: \", model_inputs.input_ids.shape)\n",
    "print()\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# now the generated_ids containts also the input prompt. We remove it\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(\"generated_ids[0][:5]: \", generated_ids[0][:5], \"...\")\n",
    "print(\"generated_ids[0].shape: \", generated_ids[0].shape)\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\nresponse:\\n\", response, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "### Usage Recommendations\n",
    "\n",
    "We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\n",
    "\n",
    "1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n",
    "2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n",
    "3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n",
    "4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n",
    "\n",
    "Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151646,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Give me a short introduction to large language model.<｜Assistant｜><think>\n",
      "\n",
      "model_inputs:  dict_keys(['input_ids', 'attention_mask'])\n",
      "model_inputs.input_ids.shape:  torch.Size([1, 16])\n",
      "\n",
      "generated_ids[0][:5]:  tensor([32313,    11,   773,   358,  1184], device='cuda:0') ...\n",
      "generated_ids[0].shape:  torch.Size([512])\n",
      "\n",
      "response:\n",
      "Okay, so I need to give a short introduction to a large language model. Hmm, where do I start? I know that large language models are AI systems designed to understand and generate human language, but I'm not entirely sure about all the details. Let me think about what I know and what I might not know.\n",
      "\n",
      "First, what exactly is a large language model? I think it's a type of AI that can understand and produce human-level text. They can read, write, speak, and generate text based on what they've been trained on. But how do they do that? I remember hearing that they use a lot of data to learn patterns in language. So, they must have massive datasets to work with.\n",
      "\n",
      "I also remember that some models are better at certain tasks than others. Like, while they can read and write, they might struggle with very complex or creative writing. But I'm not sure how they handle that. Maybe they use more advanced algorithms or techniques?\n",
      "\n",
      "Wait, I think there are different types of large language models. There are open-source ones like GPT, which I've heard of. Are there also proprietary models? I believe there are open-source models like the one developed by OpenAI, which is called OpenAI GPT-3. But are there others? I think there are models trained on different datasets, like Chinese, English, etc., which might make them better at certain languages.\n",
      "\n",
      "I'm also curious about how they work under the hood. Do they have neural networks? I think neural networks are a big part of how these models process information. They have layers of neurons, and they learn patterns through backpropagation. That makes sense because neural networks are inspired by the human brain's structure.\n",
      "\n",
      "I wonder about their applications. Beyond just text generation, are they used in other areas like music or art? I've heard of AI in music, like AI-generated music or even AI for composing songs. Maybe large language models can be used to create similar outputs based on text, but I'm not sure how feasible that is.\n",
      "\n",
      "Another thought: how do these models handle creativity? I mean, they can generate text, but is it just repeating what they've been trained on, or can they come up with original content? Maybe with more training data or specific prompts, they can generate more creative outputs. But I'm not entirely certain.\n",
      "\n",
      "I should also consider the challenges associated with these models. Since they rely on vast amounts of data, maintaining them might be difficult. If the data is outdated or incomplete\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True, # generation promp = <think>\\n at the end od the prompt\n",
    ")\n",
    "print(text)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "print(\"model_inputs: \", model_inputs.keys())\n",
    "print(\"model_inputs.input_ids.shape: \", model_inputs.input_ids.shape)\n",
    "print()\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(\"generated_ids[0][:5]: \", generated_ids[0][:5], \"...\")\n",
    "print(\"generated_ids[0].shape: \", generated_ids[0].shape)\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\nresponse:\\n\", response, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Trying 4bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for 10 generations:  84.79\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "prompt = \"Write a story about a dragon and a knight of approximately 300 words.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "t0 = time.time()\n",
    "for _ in range(10):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, # generation promp = <think>\\n at the end od the prompt\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time required for 10 generations: \", round(t1 - t0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for 10 generations:  101.36\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "model_4bit.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(10):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, # generation promp = <think>\\n at the end od the prompt\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model_4bit.device)\n",
    "    generated_ids = model_4bit.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time required for 10 generations: \", round(t1 - t0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory foorprint (in bytes): 3554176256\n",
      "memory foorprint (in bytes): 1588882688\n"
     ]
    }
   ],
   "source": [
    "print(\"memory foorprint (in bytes):\", model.get_memory_footprint(), end=\"\\n\")\n",
    "print(\"memory foorprint (in bytes):\", model_4bit.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
