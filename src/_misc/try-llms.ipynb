{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen/Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "model_inputs:  dict_keys(['input_ids', 'attention_mask'])\n",
      "model_inputs.input_ids.shape:  torch.Size([1, 39])\n",
      "\n",
      "generated_ids[0][:5]:  tensor([  32, 3460, 4128, 1614,  374], device='cuda:0') ...\n",
      "generated_ids[0].shape:  torch.Size([116])\n",
      "\n",
      "response:\n",
      "A large language model is a type of artificial intelligence that can generate human-like text based on the input it receives. These models are designed to understand and respond to complex questions and provide detailed information in various fields such as natural language processing, machine translation, and chatbot development. Large language models have been trained on vast amounts of data, including books, articles, and videos, to enable them to produce coherent and contextually appropriate responses. They are often used in applications like virtual assistants, chatbots, and language translation systems, where they help users with their inquiries or tasks.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False, # the output is not tokenized, we have just text\n",
    "    add_generation_prompt=True, # generation promp = <|im_start|>assistant at the end od the prompt\n",
    ")\n",
    "print(text)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "print(\"model_inputs: \", model_inputs.keys())\n",
    "print(\"model_inputs.input_ids.shape: \", model_inputs.input_ids.shape)\n",
    "print()\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# now the generated_ids containts also the input prompt. We remove it\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(\"generated_ids[0][:5]: \", generated_ids[0][:5], \"...\")\n",
    "print(\"generated_ids[0].shape: \", generated_ids[0].shape)\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\nresponse:\\n\", response, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "### Usage Recommendations\n",
    "\n",
    "We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\n",
    "\n",
    "1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n",
    "2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n",
    "3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n",
    "4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n",
    "\n",
    "Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151646,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Give me a short introduction to large language model.<｜Assistant｜><think>\n",
      "\n",
      "model_inputs:  dict_keys(['input_ids', 'attention_mask'])\n",
      "model_inputs.input_ids.shape:  torch.Size([1, 16])\n",
      "\n",
      "generated_ids[0][:5]:  tensor([32313,    11,   773,   358,  1184], device='cuda:0') ...\n",
      "generated_ids[0].shape:  torch.Size([512])\n",
      "\n",
      "response:\n",
      "Okay, so I need to write a short introduction about Large Language Models. Hmm, where do I start? I know that LLMs are these big AI systems that can understand and generate human language. But what exactly are they? I think they're models that can process vast amounts of text data, right? They're used in various applications like chatbots, writing assistance, maybe even creative writing. \n",
      "\n",
      "Wait, how do they work? I remember they take in a lot of text data and use some kind of algorithm to generate new text. But I'm not entirely sure about the specifics. Maybe they learn from data, so they can generate coherent text based on that data. Oh, and they can be trained on specific tasks, like answering questions or summarizing documents. \n",
      "\n",
      "I wonder about their capabilities. Do they understand different languages? I think they can handle multiple languages, but I'm not 100% sure. Also, how efficient are they? Can they handle large texts without issues? I guess the more data they have, the better they can generate, but I'm not certain about the limits. \n",
      "\n",
      "Another thing is their applications. I've heard they're used in education, content creation, maybe even in healthcare or law. They can help create educational materials, generate content for websites, or even assist in legal processes. That's pretty cool. \n",
      "\n",
      "But what about their limitations? I suppose they can't understand all possible languages or all kinds of texts. They might struggle with creativity or nuances in certain topics. Also, they might not be as flexible as humans in some ways. \n",
      "\n",
      "I'm also thinking about the technology behind them. Are they based on neural networks? I think so, using something like GPT, which stands for Generalized Predictive State Model. GPTs are known for their ability to generate text, but they might have some limitations. \n",
      "\n",
      "Wait, how do LLMs handle context? I think they can understand the context of the text they're given. For example, if I tell them a story, they can continue it. Or if I ask a question, they can provide an answer based on the data they've been trained on. That's pretty powerful. \n",
      "\n",
      "But can they handle very complex or creative tasks? I'm not sure. They might not be able to write poetry or create original stories, but they can generate coherent and somewhat meaningful text. \n",
      "\n",
      "I also recall that LLMs can be trained on large datasets, which makes them more accurate. So, the more data\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True, # generation promp = <think>\\n at the end od the prompt\n",
    ")\n",
    "print(text)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "print(\"model_inputs: \", model_inputs.keys())\n",
    "print(\"model_inputs.input_ids.shape: \", model_inputs.input_ids.shape)\n",
    "print()\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(\"generated_ids[0][:5]: \", generated_ids[0][:5], \"...\")\n",
    "print(\"generated_ids[0].shape: \", generated_ids[0].shape)\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\nresponse:\\n\", response, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Trying 4bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for 10 generations:  84.65\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "prompt = \"Write a story about a dragon and a knight of approximately 300 words.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "t0 = time.time()\n",
    "for _ in range(10):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, # generation promp = <think>\\n at the end od the prompt\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time required for 10 generations: \", round(t1 - t0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for 10 generations:  96.1\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "model_4bit.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(10):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, # generation promp = <think>\\n at the end od the prompt\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model_4bit.device)\n",
    "    generated_ids = model_4bit.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time required for 10 generations: \", round(t1 - t0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory footprint (in bytes): 3554176256\n",
      "memory footprint (in bytes): 1588882688\n"
     ]
    }
   ],
   "source": [
    "print(\"memory footprint (in bytes):\", model.get_memory_footprint(), end=\"\\n\")\n",
    "print(\"memory footprint (in bytes):\", model_4bit.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
