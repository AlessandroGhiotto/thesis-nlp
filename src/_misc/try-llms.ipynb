{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen/Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "model_inputs:  dict_keys(['input_ids', 'attention_mask'])\n",
      "model_inputs.input_ids.shape:  torch.Size([1, 39])\n",
      "\n",
      "generated_ids[0][:5]:  tensor([95456,     0,   362,  3460,  4128], device='cuda:0') ...\n",
      "generated_ids[0].shape:  torch.Size([257])\n",
      "\n",
      "response:\n",
      "Certainly! A large language model (LLM) is a machine learning model that can generate human-like text based on the input it receives. These models are designed to be highly versatile and adaptable, allowing them to process and generate text in a wide range of domains, from science and engineering to art and entertainment.\n",
      "\n",
      "Leveraging advancements in deep learning techniques and natural language processing, LLMs have achieved impressive results in tasks such as summarization, translation, writing, and even creative writing. They can produce coherent and contextually appropriate responses, which makes them particularly useful for various applications including but not limited to:\n",
      "\n",
      "1. **Text-to-Text Generation**: Generating new text that closely mirrors the original content.\n",
      "2. **Translation**: Translating text between different languages.\n",
      "3. **Writing Assistance**: Providing ideas or prompts for generating written work.\n",
      "4. **Artistic Expression**: Creating visual representations or compositions based on textual descriptions.\n",
      "5. **Research and Documentation**: Organizing and synthesizing information from multiple sources into cohesive texts.\n",
      "\n",
      "The development of LLMs has significantly transformed how we interact with technology, enhancing productivity, creativity, and accessibility across many industries. As technology continues to advance, we can expect LLMs to play an increasingly important role in shaping our digital experiences.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False, # the output is not tokenized, we have just text\n",
    "    add_generation_prompt=True, # generation promp = <|im_start|>assistant at the end od the prompt\n",
    ")\n",
    "print(text)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "print(\"model_inputs: \", model_inputs.keys())\n",
    "print(\"model_inputs.input_ids.shape: \", model_inputs.input_ids.shape)\n",
    "print()\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# now the generated_ids containts also the input prompt. We remove it\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(\"generated_ids[0][:5]: \", generated_ids[0][:5], \"...\")\n",
    "print(\"generated_ids[0].shape: \", generated_ids[0].shape)\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\nresponse:\\n\", response, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "### Usage Recommendations\n",
    "\n",
    "We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\n",
    "\n",
    "1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n",
    "2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n",
    "3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n",
    "4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n",
    "\n",
    "Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151646,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Give me a short introduction to large language model.<｜Assistant｜><think>\n",
      "\n",
      "model_inputs:  dict_keys(['input_ids', 'attention_mask'])\n",
      "model_inputs.input_ids.shape:  torch.Size([1, 16])\n",
      "\n",
      "generated_ids[0][:5]:  tensor([32313,    11,   773,   358,  1184], device='cuda:0') ...\n",
      "generated_ids[0].shape:  torch.Size([512])\n",
      "\n",
      "response:\n",
      "Okay, so I need to write a short introduction to a large language model. Hmm, where do I start? I'm not entirely sure what a large language model is, but I think it's some kind of AI that can understand and generate human language. Maybe something like ChatGPT? I've heard of that before. \n",
      "\n",
      "Alright, so first, I should probably define what a large language model is. It's an AI that can generate text, right? Like, if I tell it to write a essay or a story, it can do that. But what makes it \"large\"? I guess it's because it can handle a wide range of topics and languages. It's powerful, but maybe it's also complex and hard to control.\n",
      "\n",
      "I should think about the main features. It's probably very efficient, able to process large amounts of data quickly. It can also adapt to different languages, which is cool because it can help with translation or writing in other languages. Maybe it has a lot of resources, like pre-trained models, which means it can learn from a lot of data without needing to retrain each time.\n",
      "\n",
      "Another thing is that it's versatile. It can be used for various tasks like summarization, creative writing, even creative problem-solving. That's pretty broad. But I'm not sure about the limitations. Maybe it's not as accurate as human language, especially in complex or nuanced situations. Also, it might not understand context as well as humans do.\n",
      "\n",
      "I should consider the purpose behind using a large language model. It's probably used in education, research, and maybe even in creative industries like writing or design. It can help students with assignments or researchers in generating research papers. But I'm not sure how it handles emotions or sarcasm. It might not be able to express those nuances as well as humans.\n",
      "\n",
      "I wonder about the ethical implications. Since it's an AI, there might be issues related to bias, privacy, and safety. I should mention that it's important to be cautious about how it's used and the potential consequences of its misuse.\n",
      "\n",
      "Now, thinking about its structure. An introduction should give a overview, maybe talk about its capabilities, limitations, and potential applications. It should also touch on its impact on society. Maybe end with a statement about the future of AI and its role in human creativity.\n",
      "\n",
      "Wait, did I miss anything? Oh, right, it's a tool that can be integrated into various fields, but it's also something that can be controlled and manipulated. So, it's\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True, # generation promp = <think>\\n at the end od the prompt\n",
    ")\n",
    "print(text)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "print(\"model_inputs: \", model_inputs.keys())\n",
    "print(\"model_inputs.input_ids.shape: \", model_inputs.input_ids.shape)\n",
    "print()\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(\"generated_ids[0][:5]: \", generated_ids[0][:5], \"...\")\n",
    "print(\"generated_ids[0].shape: \", generated_ids[0].shape)\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\nresponse:\\n\", response, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Trying 4bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for 10 generations:  85.48\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "prompt = \"Write a story about a dragon and a knight of approximately 300 words.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "t0 = time.time()\n",
    "for _ in range(10):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, # generation promp = <think>\\n at the end od the prompt\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time required for 10 generations: \", round(t1 - t0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for 10 generations:  95.4\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='flash_attention_2',\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "model_4bit.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(10):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, # generation promp = <think>\\n at the end od the prompt\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model_4bit.device)\n",
    "    generated_ids = model_4bit.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time required for 10 generations: \", round(t1 - t0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory foorprint (in bytes): 3554176256\n",
      "memory foorprint (in bytes): 1588882688\n"
     ]
    }
   ],
   "source": [
    "print(\"memory foorprint (in bytes):\", model.get_memory_footprint(), end=\"\\n\")\n",
    "print(\"memory foorprint (in bytes):\", model_4bit.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
