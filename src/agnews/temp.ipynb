{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CHANGE WORKING DIRECTORY TO ROOT\n",
    "current_dir = os.path.basename(os.getcwd())\n",
    "if current_dir == \"src\":\n",
    "    os.chdir(\"..\") # Move up by 1\n",
    "elif os.path.basename(os.getcwd()) == \"bai-thesis-nlp\":  \n",
    "    pass # If already at root, stay there\n",
    "else:\n",
    "    os.chdir(\"../..\") # Move up by 2 otherwise\n",
    "    \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import re\n",
    "from src._utils._helpers import log_synthetic_data, response2json, get_response, set_seed, clear_cuda_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000_train</td>\n",
       "      <td>Hartford executive #39;s stock sale probed</td>\n",
       "      <td>New York AG is investigating the timing of a s...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001_train</td>\n",
       "      <td>Cool Batman Photo!</td>\n",
       "      <td>In related news, it was announced yesterday th...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002_train</td>\n",
       "      <td>Stocky Monkey in Himalayas Becomes Newest Prim...</td>\n",
       "      <td>Scientists from India working in the Himalayas...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003_train</td>\n",
       "      <td>Bush, Kerry Don''t Worry About Tech</td>\n",
       "      <td>Election Day is less than two weeks away, but ...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004_train</td>\n",
       "      <td>The Teeming Crowd in Video Games</td>\n",
       "      <td>Jeffrey Griffiths, the president and chief exe...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                                                  1  \\\n",
       "0  1000_train         Hartford executive #39;s stock sale probed   \n",
       "1  1001_train                                 Cool Batman Photo!   \n",
       "2  1002_train  Stocky Monkey in Himalayas Becomes Newest Prim...   \n",
       "3  1003_train                Bush, Kerry Don''t Worry About Tech   \n",
       "4  1004_train                   The Teeming Crowd in Video Games   \n",
       "\n",
       "                                                text     label  \n",
       "0  New York AG is investigating the timing of a s...  Business  \n",
       "1  In related news, it was announced yesterday th...  Sci/Tech  \n",
       "2  Scientists from India working in the Himalayas...  Sci/Tech  \n",
       "3  Election Day is less than two weeks away, but ...  Sci/Tech  \n",
       "4  Jeffrey Griffiths, the president and chief exe...  Sci/Tech  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: Business, Sci/Tech, Sports, World\n",
      "Label: Business\n",
      "Text: New York AG is investigating the timing of a sale by Thomas Marra; company reports higher 3Q. NEW YORK (Reuters) - The Hartford Financial Services Group Inc.\n",
      "\n",
      "Label: Sci/Tech\n",
      "Text: In related news, it was announced yesterday that Warner Bros. Interactive Entertainment, DC Comics and Electronic Arts will bring a Batman Begins videogame tie-in.\n",
      "\n",
      "Label: Sports\n",
      "Text: Lindsay Davenport #39;s world number one ranking is in doubt after she failed to make the finals of the WTA Tour Championships in Los Angeles.\n",
      "\n",
      "Label: World\n",
      "Text:  JERUSALEM (Reuters) - Israeli Prime Minister Ariel Sharon  accused far-rightists Sunday of trying to incite civil war over  his plan to withdraw from the occupied Gaza Strip and called  for measures to curb such groups.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the folder to save the synthetic data\n",
    "folder_name = \"synthetic_data/logs\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# file where the logs will be saved\n",
    "log_file_path = folder_name + \"/agnews_log.json\"\n",
    "RECREATE_LOG = False\n",
    "if os.path.exists(log_file_path) and RECREATE_LOG:\n",
    "    os.remove(log_file_path) # recreate from scratch\n",
    "\n",
    "# DEVICE\n",
    "device = 'cuda:0'\n",
    "\n",
    "# DATA\n",
    "df = pd.read_csv(\"real_data/train/agnewstrainAll.csv\")\n",
    "df = df.rename(columns={\"2\": \"text\", \"3\": \"label\"})\n",
    "display(df.head())\n",
    "\n",
    "labels_lst = df['label'].unique()\n",
    "labels_str = \", \".join(labels_lst)\n",
    "labels_str_bullet = \"\\n\".join([f\"- **{name}**\" for name in labels_lst])\n",
    "print(f\"Labels: {labels_str}\")\n",
    "\n",
    "# Print the first example for each label\n",
    "examples = []\n",
    "for label in labels_lst:\n",
    "    example = (df[df['label'] == label].iloc[0]).loc['text']\n",
    "    examples.append(example)\n",
    "    print(f\"Label: {label}\\nText: {example}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src._utils._generate_dataset import generate_synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_length': 20, 'max_new_tokens': None, 'min_length': 0, 'min_new_tokens': None, 'early_stopping': False, 'max_time': None, 'stop_strings': None, 'do_sample': True, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'dola_layers': None, 'use_cache': True, 'cache_implementation': None, 'cache_config': None, 'return_legacy_cache': None, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.95, 'min_p': None, 'typical_p': 1.0, 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'forced_decoder_ids': None, 'sequence_bias': None, 'token_healing': False, 'guidance_scale': None, 'low_memory': None, 'watermarking_config': None, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'output_logits': None, 'return_dict_in_generate': False, 'pad_token_id': 151643, 'bos_token_id': 151646, 'eos_token_id': 151643, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id': None, 'is_assistant': False, 'num_assistant_tokens': 20, 'num_assistant_tokens_schedule': 'constant', 'assistant_confidence_threshold': 0.4, 'prompt_lookup_num_tokens': None, 'max_matching_ngram_size': None, 'assistant_early_exit': None, 'assistant_lookbehind': 10, 'target_lookbehind': 10, 'disable_compile': False, 'generation_kwargs': {}, '_from_model_config': True, 'transformers_version': '4.49.0'}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    quantization_config=quantization_config,  # load in 4-bit quantization\n",
    "    # if I want to add other model parameters, I can add them here\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(model.generation_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_load_in_4bit': True, 'load_in_4bit': True}\n"
     ]
    }
   ],
   "source": [
    "if hasattr(model.config, \"quantization_config\"):\n",
    "    print(model.config.quantization_config.to_diff_dict())\n",
    "\n",
    "else:\n",
    "    print(\"Model does not have quantization_config\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_load_in_4bit': True, 'load_in_4bit': True}\n"
     ]
    }
   ],
   "source": [
    "res = model.config.quantization_config.to_diff_dict() \\\n",
    "        if hasattr(model.config, \"quantization_config\") else None\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
