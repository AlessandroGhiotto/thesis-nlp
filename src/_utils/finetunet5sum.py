# -*- coding: utf-8 -*-
"""finetuneT5.ipynb

Automatically generated by Colab.

"""


from datasets import load_dataset
from transformers import AutoTokenizer
from datasets import concatenate_datasets
from transformers import T5ForConditionalGeneration
import evaluate
import nltk
import numpy as np
from nltk.tokenize import sent_tokenize
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
import accelerate
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import f1_score
from statistics import mean
from transformers import DataCollatorForSeq2Seq
from transformers import DataCollatorForSeq2Seq


nltk.download("punkt")

metric = evaluate.load("f1")
model_id="t5-base"

datasettrain = load_dataset("csv",data_files='',encoding='utf-8')
datatesttest = load_dataset("csv",data_files='',encoding='utf-8')


# Load tokenizer of FLAN-t5-base
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = T5ForConditionalGeneration.from_pretrained(model_id).to('cuda')

tokenized_inputs = datasettrain['train'].map(lambda x: tokenizer(x["text"], truncation=True), batched=True, remove_columns=["text", "label"])
max_source_length = max([len(x) for x in tokenized_inputs["input_ids"]])

tokenized_targetstest = datatesttest['train'].map(lambda x: tokenizer(x["label"], truncation=True), batched=True, remove_columns=["text", "label"])
max_target_length = max([len(str(x)) for x in tokenized_targetstest["input_ids"]])



def preprocess_function(sample,padding="max_length"):

    inputs = ["classify: " + item for item in sample["text"]]
    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)
    labels = tokenizer(text_target=sample["label"], max_length=max_target_length, padding=padding, truncation=True)

    if padding == "max_length":
        labels["input_ids"] = [
            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
        ]

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset_train = datasettrain.map(preprocess_function, batched=True, remove_columns=["text", "label"])
tokenized_dataset_test = datatesttest.map(preprocess_function, batched=True, remove_columns=["text", "label"])


# helper function to postprocess text
def postprocess_text(preds, labels):
    labels = [label.strip().lower().replace('_',' ').strip() for label in labels]
    preds = [pred.strip().lower().replace('_',' ').strip() for pred in preds]

    return preds, labels


def mappingfiles(mappingfile):

    label2id = {}
    with open(mappingfile) as f:
        lines = f.readlines()

    for line in lines:
        labelid = int(line.split('\t')[0])
        label = line.split('\t')[1].strip()
        label2id[label.lower().replace('_',' ').strip()] = labelid

    return label2id

def compute_metricsmulticl(eval_preds):
    preds, labels = eval_preds
    print('all pred: ',preds)
    print('all labels: ',labels)
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)


    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
    clpreds = []
    clactual = []
    nonexistpreds = []
    label2id = mappingfiles('')

    for i in range(0,len(decoded_preds)):


      if  decoded_preds[i] in list(set(decoded_labels)):
        clpreds.append(label2id[decoded_preds[i]])
        clactual.append(label2id[decoded_labels[i]])

      else:
        nonexistpreds.append(decoded_preds[i])



    result = metric.compute(predictions=clpreds, references=clactual,average='macro')
    microf1 = metric.compute(predictions=clpreds, references=clactual,average='micro')
    nonclassified = len(nonexistpreds)
    notexisting = (nonclassified * 100) / len(decoded_preds)
    print('nonexisting: ',notexisting)
    print('micro: ',microf1)

    return result


def computemultilabel(eval_preds):
  preds, labels = eval_preds
  uniquelabels = []

  print(uniquelabels)
  if isinstance(preds, tuple):
    preds = preds[0]
  nonexistpreds = []
  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)


  decodedlabelslist = []
  for label in decoded_labels:
    #print('label here: ',label)
    perrowl = []
    for l in label.strip().lower().split('and'):
      l = l.strip()


      if l in uniquelabels:
        perrowl.append(l)

    decodedlabelslist.append(perrowl)


  decodedlabelpredictionslist = []
  for predd in decoded_preds:
    perrowp = []
    if 'and' in predd:
      for p in predd.strip().lower().split('and'):
        p = p.strip().lower().replace(',','').strip()
        if p in uniquelabels:
          if predd not in perrowp:
            perrowp.append(p)
        else:
          nonexistpreds.append(p)
    else:
      predd = predd.lower().replace(',','').strip()
      if predd in uniquelabels:
        if predd not in perrowp:
          perrowp.append(predd)
      else:
        nonexistpreds.append(predd)


    decodedlabelpredictionslist.append(perrowp)

  clpredictions = []
  clactual = []
  for i in range(0,len(decodedlabelpredictionslist)):
    if decodedlabelpredictionslist[i] != []:
      clpredictions.append(decodedlabelpredictionslist[i])
      clactual.append(decodedlabelslist[i])


  mlb = MultiLabelBinarizer()
  mlb.fit([uniquelabels])
  truebinary = mlb.transform(clactual)
  predictedbinary = mlb.transform(clpredictions)

  microf1 = f1_score(truebinary, predictedbinary, average='micro')
  result = f1_score(truebinary, predictedbinary, average='macro')

  print('microscore: ',microf1)
  print('macroscore: ',result)
  print(nonexistpreds)
  print(len(nonexistpreds))


  return result


label_pad_token_id = -100
# Data collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model,
    label_pad_token_id=label_pad_token_id,
    pad_to_multiple_of=8
)




# Define training args
training_args = Seq2SeqTrainingArguments(
    output_dir='/content/modelirony',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    fp16=False,
    learning_rate=5e-5,
    num_train_epochs=2,
    logging_dir="content/logs",
    logging_strategy="steps",
    logging_steps=500,
    save_total_limit=2,
    report_to="tensorboard",
    push_to_hub=False,

)

# Create Trainer instance
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset_train["train"],
    eval_dataset=tokenized_dataset_test["train"],
    compute_metrics=compute_metricsmulticl,
)


trainer.train()
trainer.evaluate()
predictions = trainer.predict(tokenized_dataset_test["train"])
preds = np.argmax(predictions.predictions)

